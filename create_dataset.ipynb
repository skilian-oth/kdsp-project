{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets from the twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = \"\"\n",
    "\n",
    "hashtag = \"Paris\"\n",
    "dataset_name = hashtag\n",
    "num_tweets = 1000\n",
    "allow_retweets = False\n",
    "\n",
    "# Create a folder for the dataset if it doesn't exist\n",
    "try:\n",
    "    import os\n",
    "    os.mkdir(\"data/\" + dataset_name)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Make the request to the API\n",
    "# We search for tweets containing the hashtag and their author\n",
    "\n",
    "\n",
    "# Get the current mininumum tweet id in a dataset\n",
    "def get_min_index(hashtag):\n",
    "    tweets_df = pd.read_csv(\"data/\" + hashtag + \"/tweets.csv\")\n",
    "    min_index = tweets_df[\"id\"].min()\n",
    "    return min_index\n",
    "\n",
    "\n",
    "\n",
    "# Store the response in a json file\n",
    "def get_tweets(hashtag, max_id=None):\n",
    "\n",
    "    # Build the max_id parameter\n",
    "    max_id_string = \"\"\n",
    "    if max_id is not None:\n",
    "        max_id_string = \"&until_id=\" + str(max_id)\n",
    "\n",
    "    # Build the max_results parameter\n",
    "    max_results = 100\n",
    "    if num_tweets < max_results:\n",
    "        max_results = num_tweets\n",
    "    max_results_string = \"&max_results=\" + str(max_results)\n",
    "\n",
    "    # Build the allow_retweets parameter\n",
    "    retweet_string = \"\"\n",
    "    if not allow_retweets:\n",
    "        retweet_string = \"%20-is%3Aretweet\"\n",
    "\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query=%23\" + hashtag + retweet_string + max_id_string + max_results_string + \"&expansions=author_id&tweet.fields=id,created_at,text,author_id,in_reply_to_user_id,public_metrics,possibly_sensitive,lang&user.fields=id,created_at,name,username,protected,verified,profile_image_url,location,url,description,pinned_tweet_id,public_metrics\"\n",
    "    res = requests.get(url, headers={\"Authorization\": \"Bearer \" + bearer_token})\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if (res.json().get(\"errors\")):\n",
    "        print(res.json().get(\"errors\"))\n",
    "\n",
    "    # Store the response in a json object\n",
    "    json_object = res.json()\n",
    "\n",
    "    # Change the int ids into strings for the tweets\n",
    "    for i in range(len(json_object[\"data\"])):\n",
    "        json_object[\"data\"][i][\"id\"] = str(json_object[\"data\"][i][\"id\"])\n",
    "        json_object[\"data\"][i][\"author_id\"] = str(json_object[\"data\"][i][\"author_id\"])\n",
    "\n",
    "\n",
    "    # Change the int ids into strings for the includes.users\n",
    "    for i in range(len(json_object[\"includes\"][\"users\"])):\n",
    "        json_object[\"includes\"][\"users\"][i][\"id\"] = str(json_object[\"includes\"][\"users\"][i][\"id\"])\n",
    "\n",
    "    # Save the results to a json file\n",
    "    with open(\"data/\" + dataset_name + \"/response.json\", \"w\") as f:\n",
    "        json_string = json.dumps(json_object)\n",
    "        f.write(json_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Extract the tweets from the response\n",
    "def extract_tweets(dataset_name):\n",
    "\n",
    "    # Load the json file\n",
    "    with open(\"data/\" + dataset_name + \"/response.json\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the response to a pandas dataframe\n",
    "    res_df = pd.DataFrame(data[\"data\"])\n",
    "\n",
    "    # If it exists, load the tweets.csv file\n",
    "    try:\n",
    "        tweets_df = pd.read_csv(\"data/\" + dataset_name + \"/tweets.csv\")\n",
    "    except:\n",
    "        tweets_df = pd.DataFrame()\n",
    "\n",
    "    # Add the new tweets to the dataframe\n",
    "    tweets_df = pd.concat([tweets_df, res_df])\n",
    "    \n",
    "    # Save the dataframe to the csv file\n",
    "    tweets_df.to_csv(\"data/\" + dataset_name + \"/tweets.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Get the authors of tweets\n",
    "\n",
    "# Extract the authors of the tweets from the reqest\n",
    "def extract_authors(dataset_name):\n",
    "    \n",
    "    # Load the json file\n",
    "    with open(\"data/\" + dataset_name + \"/response.json\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the response to a pandas dataframe\n",
    "    res_df = pd.DataFrame(data[\"includes\"][\"users\"])\n",
    "\n",
    "    # If it exists, load the authors.csv file\n",
    "    try:\n",
    "        authors_df = pd.read_csv(\"data/\" + dataset_name + \"/authors.csv\")\n",
    "    except:\n",
    "        authors_df = pd.DataFrame()\n",
    "\n",
    "    # Add the new authors to the dataframe\n",
    "    authors_df = pd.concat([authors_df, res_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Save the authors to a csv file\n",
    "    authors_df.to_csv(\"data/\" + dataset_name + \"/authors.csv\", index=False)\n",
    "\n",
    "# Call the API to get the authors of the tweets\n",
    "def get_authors(author_id_list):\n",
    "    \n",
    "    # Generate a string of author ids\n",
    "    autor_string = \"\"\n",
    "    for author_id in author_id_list:\n",
    "        autor_string += str(author_id) + \",\"\n",
    "    autor_string = autor_string[:-1] # remove last comma\n",
    "\n",
    "    # request the listed authors to the API\n",
    "    url = \"https://api.twitter.com/2/users?ids=\" + autor_string + \"&user.fields=id,created_at,name,username,verified,profile_image_url,location,description,pinned_tweet_id\"\n",
    "    res = requests.get(url, headers={\"Authorization\": \"Bearer \" + bearer_token})\n",
    "\n",
    "    # Check if there is an error\n",
    "    if res.json().get(\"errors\"):\n",
    "        print(res.json()[\"errors\"])\n",
    "\n",
    "    # convert the response to a dataframe\n",
    "    res_df = pd.DataFrame(res.json()[\"data\"])\n",
    "\n",
    "    # Save the dataframe to a csv file in the right folder\n",
    "    res_df.to_csv(\"data/\" + dataset_name + \"/authors.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Some tests on the data\n",
    "\n",
    "# Check if we know all the authors of the tweets\n",
    "def check_authors(dataset_name):\n",
    "\n",
    "    res = True\n",
    "\n",
    "    # Load the tweets file\n",
    "    tweets_df = pd.read_csv(\"data/\" + dataset_name + \"/tweets.csv\")\n",
    "\n",
    "    # Get the list of author ids\n",
    "    author_id_list = tweets_df[\"author_id\"].unique()\n",
    "\n",
    "    # Check if the authors are in the authors file\n",
    "    authors_df = pd.read_csv(\"data/\" + dataset_name + \"/authors.csv\")\n",
    "\n",
    "    for author_id in author_id_list:\n",
    "        if author_id not in authors_df[\"id\"].unique():\n",
    "            print(\"Author id \" + str(author_id) + \" not found in the authors file\")\n",
    "            res = False\n",
    "\n",
    "    if (res):\n",
    "        print(\"All the tweets authors are in the authors file\")\n",
    "\n",
    "# Look for duplicate tweets\n",
    "def check_duplicate_tweets(dataset_name):\n",
    "\n",
    "    # Load the tweets file\n",
    "    tweets_df = pd.read_csv(\"data/\" + dataset_name + \"/tweets.csv\")\n",
    "\n",
    "    # Get the duplicates\n",
    "    duplicates = tweets_df.duplicated(subset=[\"id\"])\n",
    "\n",
    "    # Check if there are duplicates\n",
    "    if (duplicates.any()):\n",
    "        print(\"There are duplicate tweets:\")\n",
    "        print(tweets_df[duplicates])\n",
    "    else:\n",
    "        print(\"There are no duplicate tweets\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the first batch of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min from the first batch: 1536028215709511680\n"
     ]
    }
   ],
   "source": [
    "# First batch of tweets\n",
    "get_tweets(hashtag)\n",
    "extract_tweets(dataset_name)\n",
    "extract_authors(dataset_name)\n",
    "min_id = get_min_index(dataset_name)\n",
    "print(\"Min from the first batch:\", min_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import one more batch of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min from the previous batch: 1535854506684125185\n",
      "Min from this batch: 1535816097919811585\n"
     ]
    }
   ],
   "source": [
    "# One more batch of tweets\n",
    "min_id = get_min_index(dataset_name)\n",
    "print(\"Min from the previous batch:\", min_id)\n",
    "get_tweets(hashtag, min_id)\n",
    "extract_tweets(dataset_name)\n",
    "extract_authors(dataset_name)\n",
    "min_id = get_min_index(dataset_name)\n",
    "print(\"Min from this batch:\", min_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the dataset is correctly created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the tweets authors are in the authors file\n",
      "There are no duplicate tweets\n",
      "Number of tweets: 1500\n",
      "Number of authors: 1157\n"
     ]
    }
   ],
   "source": [
    "# Check if we know all the authors of the tweets\n",
    "check_authors(dataset_name)\n",
    "check_duplicate_tweets(dataset_name)\n",
    "\n",
    "tweets_df = pd.read_csv(\"data/\" + dataset_name + \"/tweets.csv\")\n",
    "print('Number of tweets:', len(tweets_df))\n",
    "\n",
    "authors_df = pd.read_csv(\"data/\" + dataset_name + \"/authors.csv\")\n",
    "print('Number of authors:', len(authors_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1536041012849692675</td>\n",
       "      <td>The bright side.  A Sunday under the sun. The ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1379526403578806280</td>\n",
       "      <td>2022-06-12T17:41:30.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>fr</td>\n",
       "      <td>1536040869312200704</td>\n",
       "      <td>Martine Franck. \\nHenri Cartier-Bresson dessin...</td>\n",
       "      <td>{'retweet_count': 1, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>875280010877296640</td>\n",
       "      <td>2022-06-12T17:40:56.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>1536040769630388227</td>\n",
       "      <td>Мир Украине, свободу России. #paris #standwith...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1519399318557532167</td>\n",
       "      <td>2022-06-12T17:40:32.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>fr</td>\n",
       "      <td>1536040742434521088</td>\n",
       "      <td>C'est un établissement pluridisciplinaire né d...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1512584037369589761</td>\n",
       "      <td>2022-06-12T17:40:25.000Z</td>\n",
       "      <td>1.512584e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>und</td>\n",
       "      <td>1536040680367210498</td>\n",
       "      <td>#NowPlaying #Africanism @martinsolveig #house ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1390289676695654401</td>\n",
       "      <td>2022-06-12T17:40:11.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1536005494778867713</td>\n",
       "      <td>It's no longer possible to make a reservation ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>791590844050599936</td>\n",
       "      <td>2022-06-12T15:20:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>False</td>\n",
       "      <td>fr</td>\n",
       "      <td>1536005467352227841</td>\n",
       "      <td>Enlaidissement non seulement inutile mais surt...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>3407734155</td>\n",
       "      <td>2022-06-12T15:20:15.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>False</td>\n",
       "      <td>ht</td>\n",
       "      <td>1536004928069672963</td>\n",
       "      <td>✌Que des #bonnesvibes #EnCeMomentSur :\\n\\n ➡️ ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>971844112575057920</td>\n",
       "      <td>2022-06-12T15:18:07.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1536004741616246785</td>\n",
       "      <td>#FrançoiseDorléac, December 21, 1962, #Paris. ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1478002801586618374</td>\n",
       "      <td>2022-06-12T15:17:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>False</td>\n",
       "      <td>fr</td>\n",
       "      <td>1536004662767366145</td>\n",
       "      <td>Montmartre - La Rue des Saules à Paris 18e (Pa...</td>\n",
       "      <td>{'retweet_count': 1, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>1405930262651932674</td>\n",
       "      <td>2022-06-12T15:17:03.000Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     possibly_sensitive lang                   id  \\\n",
       "0                 False   en  1536041012849692675   \n",
       "1                 False   fr  1536040869312200704   \n",
       "2                 False  und  1536040769630388227   \n",
       "3                 False   fr  1536040742434521088   \n",
       "4                 False  und  1536040680367210498   \n",
       "..                  ...  ...                  ...   \n",
       "295               False   en  1536005494778867713   \n",
       "296               False   fr  1536005467352227841   \n",
       "297               False   ht  1536004928069672963   \n",
       "298               False   en  1536004741616246785   \n",
       "299               False   fr  1536004662767366145   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The bright side.  A Sunday under the sun. The ...   \n",
       "1    Martine Franck. \\nHenri Cartier-Bresson dessin...   \n",
       "2    Мир Украине, свободу России. #paris #standwith...   \n",
       "3    C'est un établissement pluridisciplinaire né d...   \n",
       "4    #NowPlaying #Africanism @martinsolveig #house ...   \n",
       "..                                                 ...   \n",
       "295  It's no longer possible to make a reservation ...   \n",
       "296  Enlaidissement non seulement inutile mais surt...   \n",
       "297  ✌Que des #bonnesvibes #EnCeMomentSur :\\n\\n ➡️ ...   \n",
       "298  #FrançoiseDorléac, December 21, 1962, #Paris. ...   \n",
       "299  Montmartre - La Rue des Saules à Paris 18e (Pa...   \n",
       "\n",
       "                                        public_metrics            author_id  \\\n",
       "0    {'retweet_count': 0, 'reply_count': 0, 'like_c...  1379526403578806280   \n",
       "1    {'retweet_count': 1, 'reply_count': 0, 'like_c...   875280010877296640   \n",
       "2    {'retweet_count': 0, 'reply_count': 0, 'like_c...  1519399318557532167   \n",
       "3    {'retweet_count': 0, 'reply_count': 0, 'like_c...  1512584037369589761   \n",
       "4    {'retweet_count': 0, 'reply_count': 0, 'like_c...  1390289676695654401   \n",
       "..                                                 ...                  ...   \n",
       "295  {'retweet_count': 0, 'reply_count': 0, 'like_c...   791590844050599936   \n",
       "296  {'retweet_count': 0, 'reply_count': 0, 'like_c...           3407734155   \n",
       "297  {'retweet_count': 0, 'reply_count': 0, 'like_c...   971844112575057920   \n",
       "298  {'retweet_count': 0, 'reply_count': 0, 'like_c...  1478002801586618374   \n",
       "299  {'retweet_count': 1, 'reply_count': 0, 'like_c...  1405930262651932674   \n",
       "\n",
       "                   created_at  in_reply_to_user_id  \n",
       "0    2022-06-12T17:41:30.000Z                  NaN  \n",
       "1    2022-06-12T17:40:56.000Z                  NaN  \n",
       "2    2022-06-12T17:40:32.000Z                  NaN  \n",
       "3    2022-06-12T17:40:25.000Z         1.512584e+18  \n",
       "4    2022-06-12T17:40:11.000Z                  NaN  \n",
       "..                        ...                  ...  \n",
       "295  2022-06-12T15:20:22.000Z                  NaN  \n",
       "296  2022-06-12T15:20:15.000Z                  NaN  \n",
       "297  2022-06-12T15:18:07.000Z                  NaN  \n",
       "298  2022-06-12T15:17:22.000Z                  NaN  \n",
       "299  2022-06-12T15:17:03.000Z                  NaN  \n",
       "\n",
       "[300 rows x 8 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"data/\" + dataset_name + \"/tweets.csv\")\n",
    "authors_df = pd.read_csv(\"data/\" + dataset_name + \"/authors.csv\")\n",
    "tweets_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e09f832a1894402aebfc063e92f06be1b97c67226102314a93108d0e0d545b4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
